{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1.1\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "#Configuration class\n",
    "class Config(object):\n",
    "    site = 'https://www.work.ua'\n",
    "    job_position = 'data+scientist'\n",
    "    n_pages = 50\n",
    "    html_elements = ['card card-hover card-visited wordwrap job-link',\n",
    "                     'card card-hover card-visited wordwrap job-link js-hot-block' ]\n",
    "\n",
    "def get_html(url):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:45.0) Gecko/20100101 Firefox/45.0'}\n",
    "        page_response = requests.get(url, headers = headers)\n",
    "        return BeautifulSoup(page_response.content, \"html.parser\")\n",
    "    except requests.exceptions.RequestException as req_er:\n",
    "        print('Request error: ',req_er)\n",
    "\n",
    "        \n",
    "\n",
    "#load the last dataset if it exists\n",
    "try:\n",
    "    latest_dataset = pd.read_csv('vacansies_dataset.csv')\n",
    "    latest_dataset_pd = latest_dataset[['id', 'title', 'time', 'requirements']].set_index('id')\n",
    "    existing_ids = latest_dataset.id\n",
    "    print('Number of existing vacancies:', len(existing_ids))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print('There is no existing dataset')\n",
    "    latest_dataset_pd = pd.DataFrame()\n",
    "    existing_ids = pd.Series()\n",
    "    \n",
    "    \n",
    "vacancies = []\n",
    "vac_dict = {}\n",
    "\n",
    "for p in range(1,Config.n_pages):\n",
    "    # Get html content\n",
    "    page_content = get_html(Config.site+'/jobs-'+Config.job_position+'/?page='+str(p))\n",
    "    # Parse job titles using html elements\n",
    "    items = []\n",
    "    for html_element in Config.html_elements:\n",
    "        items = items + page_content.find_all('div', {'class': html_element})\n",
    "\n",
    "    #get position details\n",
    "    for item in items:\n",
    "        job_link = item.find('h2').find('a').get('href')\n",
    "        job_title_full = item.find('h2').find('a').get('title')\n",
    "        job_title = re.findall(r'(.*)\\,\\sвакансія\\sвід.*', job_title_full)[0]\n",
    "        #parse job_link to get id\n",
    "        job_id = re.match(r'\\/jobs\\/(\\d{7})', job_link)\n",
    "        if job_id:\n",
    "            job_id = job_id.group(1)\n",
    "            #Check if the job_id already exist in dataset\n",
    "            if int(job_id) in existing_ids.values:\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "        #parse job title to get date\n",
    "        month_ua = {'січня': 1, 'лютого': 2, 'березня': 3, 'квітня': 4, 'травня': 5, 'червня': 6, 'липня': 7,\n",
    "                    'серпня': 8, 'вересня': 9, 'жовтня': 10, 'листопада': 11, 'грудня': 12}\n",
    "        job_date = re.match(r'.*вакансія\\sвід\\s(\\d{1,2})\\s(\\w*)\\s(\\d{4})', job_title_full).groups()\n",
    "        job_date_p = str(month_ua[job_date[1]])+'/'+job_date[0]+'/'+job_date[2]\n",
    "\n",
    "        # Get html page from job position page\n",
    "        page_content_job = get_html(Config.site+'/jobs/' + job_id + '/')\n",
    "        \n",
    "        # get job requirements and save to dictionary\n",
    "        cont = page_content_job.select(\"div.card.wordwrap ul\")\n",
    "        if len(cont)>1:\n",
    "            requirements = cont[1].get_text()\n",
    "            vac_dict.update({job_id:[job_title, job_date, job_date_p, requirements]})\n",
    "\n",
    "        else:\n",
    "            cont = page_content_job.select(\"div.card.wordwrap p\")\n",
    "            requirements = []\n",
    "            for t in cont[2:]:\n",
    "                item = t.get_text()\n",
    "                requirements = requirements + [item]\n",
    "                vac_dict.update({job_id:[job_title, job_date, job_date_p, requirements]})\n",
    "    \n",
    "# Import vacancies dictionary to pandas dataframe    \n",
    "vac_dict_pr = {'id': list(vac_dict.keys()), \n",
    "               'title': [ i[0] for i in list(vac_dict.values()) ], \n",
    "               'time': [i[2] for i in list(vac_dict.values())],\n",
    "              'requirements': [str(i[3]) for i in list(vac_dict.values())]}\n",
    "vac_df = pd.DataFrame(vac_dict_pr).set_index('id')\n",
    "\n",
    "print('Number of new vacancies:', len(vac_df))\n",
    "    \n",
    "    \n",
    "#Append new data to dataset\n",
    "new_dataset = vac_df.append(latest_dataset_pd)\n",
    "\n",
    "# Save pandas df to csv in local folder\n",
    "new_dataset.to_csv('vacansies_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyse requirements \n",
    "#Words frequency\n",
    "\n",
    "import string\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup # For HTML parsing\n",
    "import re # Regular expressions\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "from collections import Counter # Keep track of our term counts\n",
    "from stop_words import get_stop_words # Filter out stopwords, such as 'the', 'or', 'and'\n",
    "\n",
    "# load latest dataset\n",
    "try:\n",
    "    latest_dataset = pd.read_csv('vacansies_dataset.csv')\n",
    "    latest_dataset_pd = latest_dataset[['id', 'title', 'time', 'requirements']].set_index('id')\n",
    "    existing_ids = latest_dataset.id\n",
    "    print('Number of existing vacancies:', len(existing_ids))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print('There is no existing dataset')\n",
    "    latest_dataset_pd = pd.DataFrame()\n",
    "    existing_ids = pd.Series()\n",
    "\n",
    "req_words = []\n",
    "\n",
    "# clean data and tokenization\n",
    "for r in latest_dataset_pd['requirements']:\n",
    "    #print(r)\n",
    "    text = re.sub(\"[^a-zA-Zа-яА-ЯіІїЇ.+3]\",\" \", r).split() # get rid of any terms that aren't words \n",
    "    \n",
    "    table = str.maketrans('', '', string.punctuation) # create a mapping table for \n",
    "    stripped = [w.translate(table) for w in text] # Remove Punctuation\n",
    "    \n",
    "    words = [word.lower() for word in stripped] # Go to lower case\n",
    "    \n",
    "    stop_words = set(get_stop_words('ukrainian') + get_stop_words('russian') + get_stop_words('english')) # Filter out any stop words\n",
    "    cleaned_words = set([w for w in words if not w in stop_words])\n",
    "    \n",
    "    req_words = req_words + list(cleaned_words)\n",
    "    \n",
    "#count words\n",
    "req_words_freq = {}\n",
    "\n",
    "for word in req_words:\n",
    "    if word in req_words_freq:\n",
    "        req_words_freq[word] = req_words_freq[word] + 1\n",
    "    else:\n",
    "        req_words_freq[word] = 1\n",
    "        \n",
    "req_words_freq_pd = pd.Series(req_words_freq)\n",
    "\n",
    "# Filter out not important words\n",
    "not_important_words = ['na', 'xa', 'понимание', 'работа', 'умение',  '', '3', 'tools', 'processing', 'work',\n",
    "                       'good', 'language', 'plus', 'бажано', 'роботи', 'using', 'команде', 'высокие', 'др',\n",
    "                       'способность', 'работать', 'возможно', 'данных', 'практические', 'навыки', 'использованием',\n",
    "                       'различными', 'пр', 'подходов', 'принципов', 'построения', 'data', 'experience', 'skills', \n",
    "                       'опыт', 'strong', 'knowledge', 'etc', 'science', 'работы', 'understanding', 'ability', \n",
    "                       'знание', 'досвід', 'years', 'знання', 'related']\n",
    "\n",
    "req_words_freq_pd = req_words_freq_pd.reset_index(name='count')\n",
    "\n",
    "#show top 30\n",
    "req_words_freq_pd[~req_words_freq_pd['index'].isin(not_important_words)].sort_values('count' ,ascending=False)[:30]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
